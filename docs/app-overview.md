# File and Class structure

This documents the different classes that are involved in the ESP Web application and what they are responsible for.

## server.js

This is the starting file for the web portal application.

### server.js Responsibilities

1. Reads in the environment configuration (from environment vars or a config.js file)
1. Using the configuration, creates a DataAccess object for interacting with the data stores
1. Using the configuration and the DataAccess, creates an AppServer

## DataAccess.js

This module provides methods that a client can use to interact with both the CouchDB data store (where deployment information is stored) and the PostgreSQL database (where time series data is stored).

## crawler.js

This is the file that contains the code that starts the crawler that looks for files from the ESP deployments that are in need of synchronization.

### crawler.js Responsibilities

1. Reads in the environment configuration (from environment vars or a config.js file)
1. Sets up logging
1. Creates instance of DeploymentFileSync
1. Creates instance of DataAccess
1. Using DataAccess, creates instance of LogParser
1. Using DeploymentFileSync and LogParser, creates instance of CrawlerEventHandler
1. Grabs list of 'open' deployments from DataAccess
1. Loops through list and called DeploymentFileSync.syncDeployment method on each

## DeploymentFileSync.js

This code is responsible for synchronizing files from a remote FTP server to the local server.  It keeps track of things to make sure synchronization is done serially.

### DeploymentFileSync Responsibilities

1. A method is called with an incoming object that describes a deployment.
1. It does some checking and queuing of deployments to make sure things are done in a serial manner and it's not synchronizing the same deployment more than once.
1. Based on the deployment FTP info, it creates an FTP client to connect to the remote server.
1. Based on the deployment info, it finds the local directory where the files for the deployment are copied to.
1. It then opens a connection to the remote FTP server, grabs the file listing and info and looks in the local directory to compare them and see if they need to be synchronized locally.  If they do, they are added to an object that keeps track of which files need to be synch'd.
1. Then a method is called to march through the list of files marked for sync and calls a method to do the synching.
1. In this method it downloads the file to the local directory, if it happens to be a .tif file, it creates a parallel jpg version that can be used on the web.
1. Once the file is downloaded, it's added to a list of files that were updated/downloaded.
1. Note the file sync is done recursively
1. Then, once all files are sync'd, it iterates through the list of updated/downloaded files and emits and event saying it was updated along with the file stats of the file.

## CrawlerEventHandler.js

This file handles events that are generated by the processes run during file crawling.  Depending on what the event is, it will kick off other processing, like log file parsing or posting notifications to a Slack channel.

### CrawlerEventHandler Responsibilities

1. During creation, it sets up event listeners on the DeploymentFileSync object and on the LogParser object and defines handler methods for each, then it just listens.
1. If an event comes in from the DeploymentFileSync, it looks to see if the file that was updated was a log file (as defined in the deployment information), if it is, it hands the file off to the LogParser for parsing.
1. If an event comes from the LogParser, it parses the message and if it to be sent to Slack, it formats a message for Slack consumption and then adds it to an array of message to be sent to slack.
1. There is a function that runs at an interval specified in the configuration file that looks for entries in the Slack array and then posts those at that interval until the message array is empty.

## LogParser.js

This code is responsible for parsing through a log file and extracting all the pertinent information and attaching/updating that information on the deployment document in the CouchDB store (using DataAccess).

### LogParser Responsibilities

1. A method gets called that takes in a deployment, the logFile location, and the file stats associated with the log file.
1. First, it just checks to see if the LogParser has the file in it's queue, if it does, it ignores it, if it doesn't it adds it to the queue, then calls the method to process a log file entry out of the queue.
1. The process method, first checks a flag to make sure it's not currently parsing a log file.
1. If it's not parsing, it pull a file out of the queue, and makes a copy of the file in the temp directory so we know it won't change while we are parsing it (from another FTP file sync, for example).
1. Then a method is called to actually parse the log file.
1. This method first requests the most recent sample for the deployment from the data store.  This is so that we make sure we finish any sample that may have started during the last parsing, but not finished.
1. The method adds a couple of event handlers that look for errors in reading the file and to handle reaching the end of the file.
1. Then the method starts reading the file, line by line
1. It first makes sure the line does not have a continuation character and goes to next line if it does (builds a full log entry)
1. With each line, the timestamp is updated and tracked as well as the actor that is reponsible for the entry
1. It checks to make sure it's past the latest line parsed (from the previous parsing)
1. Then it calls a method to process the log entry and passes in a bunch of stuff
1. Each line has a 'header' of sorts and then a body.  The parsing method first makes sure it parses just the body portion.
1. It runs that body portion through all the pattern matchers defined in the LogParser code.
1. It does a bunch of logic depending on what kind of line was found and then put objects in arrays that are passed in to keep track of samples, images, errors, etc. that need to be updated on the deployment
1. Once the reading of the file is finished, the handler calls a method to flush any ancillary data that was found to the PostgreSQL database.
1. Once the flush is done, a method is call to clean out any duplicate ancillary data.
1. Once that cleaning is done, the ancillary data stats are updated on the deployment document in CouchDB
1. Once that is done, the deployment is updated in CouchDB with all the new information that was parsed from the file.
1. Lastly a method is called to sync the ancillary data file with the ancillary data from the database.
